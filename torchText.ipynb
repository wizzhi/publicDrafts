{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using TorchText for batched training  \n",
    "Modifiy from  https://torchtutorialstaging.z5.web.core.windows.net/beginner/text_sentiment_ngrams_tutorial.html   \n",
    "and then add the convolution layer according to https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-conv1d-for-text-classification#4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from worksapce\n",
    "\n",
    "from azureml.core import Workspace, Dataset\n",
    "subscription_id = '26ca7667-25cd-40ca-9ab0-aa0cb6fbdb97'\n",
    "resource_group = 'SBN_Analytics_Sandbox'\n",
    "workspace_name = 'azureML'\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='sample200').to_pandas_dataframe()\n",
    "testset = Dataset.get_by_name(workspace, name='test').to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from local filesystem\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#df = pd.read_csv('../../data/fullPOItemWithUnspsc/train.csv')\n",
    "dataset = pd.read_csv('../../../data/fullPOItemWithUnspsc/train_50.csv')\n",
    "testset = pd.read_csv('../../../data/fullPOItemWithUnspsc/test.csv')\n",
    "# cnt = pd.read_csv('../../data/fullPOItemWithUnspsc/full_class_cnt.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_threshold = 300   # merge those classes have n_threshold or less record as others\n",
    "n_traget = 500     # re-sample each class to n_traget record\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "for idx, row in cnt[cnt.COUNT>n_threshold].iterrows():\n",
    "    #print( \"UNSPSC:\", row.CODE, \"count:\", row.COUNT)\n",
    "    df_temp = resample( df[df.CODE == row.CODE], n_samples = n_traget, random_state=51)\n",
    "    dataset = pd.concat([dataset, df_temp], ignore_index=True)\n",
    "\n",
    "others  = cnt[cnt.COUNT <= n_threshold].CODE.tolist()\n",
    "df_temp = resample( df[ df.CODE.isin( others ) ], n_samples = n_traget, random_state=51)\n",
    "df_temp.CODE = 0\n",
    "dataset = pd.concat([dataset, df_temp], ignore_index=True)\n",
    "\n",
    "testset.CODE = testset.CODE.apply( lambda x: 0 if x in others else x)\n",
    "\n",
    "print(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                DESCRIPTION      CODE  LABEL\n",
       "0        Wealth Management Double Sided Team BC with Ti...  14111604   1205\n",
       "1        1 Pack General Flat Offset Business Card for A...  14111604   1205\n",
       "2        BC Standard Business Card 250 Box David Segura...  14111604   1205\n",
       "3        220201 Business Card Flat PDF F568084 Takahito...  14111604   1205\n",
       "4        MS Canada Limited Business Card 500 David J Ha...  14111604   1205\n",
       "...                                                    ...       ...    ...\n",
       "1013195           Material for Scenario 10 Partial receipt  40101509   7227\n",
       "1013196           Material for Scenario 10 Partial receipt  40101509   7227\n",
       "1013197           Material for Scenario 10 Partial receipt  40101509   7227\n",
       "1013198           Material for Scenario 10 Partial receipt  40101509   7227\n",
       "1013199           Material for Scenario 10 Partial receipt  40101509   7227\n",
       "\n",
       "[1013200 rows x 3 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# dataset = dataset.drop_duplicates()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "dataset[\"LABEL\"] = labelEncoder.fit_transform( dataset.CODE )\n",
    "testset[\"LABEL\"] = labelEncoder.transform( testset.CODE )\n",
    "\n",
    "dataset.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary is 604206\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = dataset.itertuples()\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for row in data_iter:\n",
    "        yield tokenizer( getattr(row, \"DESCRIPTION\") )\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "print( \"size of vocabulary is\",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67, 21111, 220, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_pipeline('test 200mA usb 苹果')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_tokens = 50\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for ( idx , row) in batch:\n",
    "        # print(row)\n",
    "        label_list.append(getattr(row, 'LABEL'))\n",
    "\n",
    "        tokens = text_pipeline(getattr(row, 'DESCRIPTION'))\n",
    " \n",
    "        # if max_tokens defined, cutting or padding tokens to max_tokens\n",
    "        if max_tokens > 0:\n",
    "            tokens = tokens+([0]* (max_tokens-len(tokens))) if len(tokens)<max_tokens else tokens[:max_tokens]\n",
    "\n",
    "        processed_text = torch.tensor(tokens, dtype=torch.int64)\n",
    " \n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        #self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 1024, kernel_size=7, padding=\"same\")\n",
    "#        self.fc1 = nn.Linear(embed_dim, 2000)\n",
    "#        self.layer1 = nn.Sequential(self.fc1, nn.ReLU(), nn.Dropout(p=0.1))\n",
    "#        self.fc2 = nn.Linear(2000, 2000)\n",
    "#        self.layer2 = nn.Sequential(self.fc2, nn.ReLU(), nn.Dropout(p=0.1))\n",
    "        self.fc3 = nn.Linear(1024, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.conv1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.conv1.bias.data.zero_()\n",
    "#        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "#        self.fc1.bias.data.zero_()\n",
    "#        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "#        self.fc2.bias.data.zero_()\n",
    "        self.fc3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc3.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "#        out = self.embedding(text, offsets)\n",
    "        out = self.embedding(text.reshape( len(offsets),  max_tokens ))\n",
    "        #out = out.mean(dim=1)\n",
    "        out = out.reshape(len(out), self.embed_dim, max_tokens)\n",
    "        out = nn.functional.relu(self.conv1(out))\n",
    "        out, _ = out.max(dim=-1)\n",
    "\n",
    "#        out = self.layer1(out)\n",
    "#        out = self.layer2(out)\n",
    "        return  self.fc3(out)\n",
    "\n",
    "\n",
    "#train_iter = AG_NEWS(split='train')\n",
    "num_class = len(labelEncoder.classes_)\n",
    "vocab_size = len(vocab)\n",
    "#emsize = 64\n",
    "emsize = 2048\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "\n",
    "#model.train()\n",
    "#t = torch.tensor([0]*100, dtype=torch.int64)\n",
    "#model( t ,[0,50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 #64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "# train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(dataset.iterrows())\n",
    "#test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "#                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    # not sure if empty cache helps to solve the OOM error\n",
    "    # torch.cuda.empty_cache()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.540\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_dataset = to_map_style_dataset(testset.iterrows())\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6545270484024043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print( evaluate(valid_dataloader) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a26fc85588032a0e1d02a9b7d8de449a1ec3ab4c5cbf0c495c9a1c458f7d6cb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
